# Enhanced üìé Multimodal & Voice Features

Complete guide to the new multimodal and voice capabilities.

## üéØ Overview

The enhanced multimodal and voice system provides:

- **üñºÔ∏è Advanced Image Processing**: Metadata extraction, optimization, resizing
- **üìÑ Document Processing**: PDF, DOCX, TXT, CSV extraction and analysis
- **üîä Audio Processing**: Duration extraction, feature analysis, format conversion
- **üé¨ Video Processing**: Frame extraction, thumbnails, metadata retrieval
- **üéôÔ∏è Text-to-Speech**: Multiple providers (Google, ElevenLabs, Azure, OpenAI)
- **üéß Speech-to-Text**: Real-time transcription with punctuation and filtering
- **üé§ Voice Sessions**: Track and manage voice interactions
- **‚è±Ô∏è Real-time Audio**: Live recording and audio level detection

## üì¶ Modules

### `multimodal_advanced.py` (680 lines)

**Core Components:**

```python
# Media types and formats
MediaType: IMAGE, DOCUMENT, AUDIO, VIDEO, TEXT
ImageFormat: JPEG, PNG, GIF, WEBP, BMP
DocumentFormat: PDF, DOCX, TXT, MD, CSV, JSON
AudioFormat: MP3, WAV, M4A, OGG, FLAC
VideoFormat: MP4, AVI, MOV, MKV, WEBM

# Data classes
MediaFile: filename, media_type, format_type, file_size, processed_at, metadata
ImageData: width, height, channels, color_space, has_alpha, exif_data
AudioData: duration_seconds, sample_rate, channels, bitrate, codec
VideoData: duration_seconds, fps, resolution, codec, bitrate, frame_count
```

**Key Classes:**

1. **ImageProcessor**
   - `validate_image()` - Check if image is valid
   - `extract_image_metadata()` - Get EXIF and image properties
   - `resize_image()` - Resize with max dimensions
   - `optimize_for_ai()` - Prepare image for AI processing

2. **DocumentProcessor**
   - `extract_text_from_pdf()` - PDF text extraction with metadata
   - `extract_text_from_docx()` - DOCX parsing
   - `extract_text_from_txt()` - Text file reading
   - `extract_csv_data()` - CSV analysis

3. **AudioProcessor**
   - `get_audio_duration()` - Duration calculation
   - `extract_audio_features()` - MFCC and RMS analysis
   - `convert_audio_format()` - Format conversion
   - `normalize_audio()` - Audio level normalization

4. **VideoProcessor**
   - `get_video_info()` - Video metadata
   - `extract_frames()` - Key frame extraction
   - `create_video_thumbnail()` - Thumbnail generation
   - `compress_video()` - Quality-based compression

5. **MultimodalManager** (Orchestrator)
   - `process_file()` - Auto-detect and process files
   - `get_processing_summary()` - Statistics and history
   - `clear_history()` - Reset processing state

### `voice_advanced.py` (650 lines)

**Core Components:**

```python
# Voice configurations
VoiceGender: MALE, FEMALE, NEUTRAL
SpeechRate: SLOW (0.7x), NORMAL (1.0x), FAST (1.3x)
VoiceProfile: FRIENDLY, PROFESSIONAL, CALM, ENERGETIC, ROBOTIC, NATURAL

# Configuration dataclasses
TTSConfig: provider, voice_name, language, speed, pitch, volume_gain, profile, use_ssml
STTConfig: provider, language, enable_punctuation, enable_profanity_filter, use_context_hints
VoicePreference: language, voice, gender, speech_rate, auto_speak, profile, accent
```

**Key Classes:**

1. **TextToSpeech**
   - Providers: Google Cloud, ElevenLabs, Azure Speech, OpenAI TTS
   - `synthesize()` - Convert text to speech audio
   - `_apply_ssml()` - Add prosody and emphasis
   - `get_available_voices()` - List provider voices
   - Speech history tracking

2. **SpeechToText**
   - Providers: Google Cloud, Azure Speech, OpenAI Whisper
   - `transcribe()` - Convert audio to text
   - `_apply_profanity_filter()` - Content filtering
   - `_add_punctuation()` - Automatic punctuation
   - Transcription history tracking

3. **VoiceSessionManager**
   - `create_session()` - Start voice interaction
   - `add_voice_message()` - Log messages to session
   - `close_session()` - End session and archive
   - `set_user_preference()` - Store voice preferences
   - `get_session_stats()` - Session statistics

4. **RealTimeAudioProcessor**
   - `start_recording()` - Begin audio capture
   - `add_audio_chunk()` - Buffer audio data
   - `stop_recording()` - Finalize and return audio
   - `get_audio_level()` - Real-time level detection
   - `detect_silence()` - Silence detection threshold

### `multimodal_voice_integration.py` (400 lines)

**Integration Layer:**

```python
class MultimodalVoiceIntegrator:
    # Streamlit UI components
    - create_multimodal_uploader()
    - create_voice_settings()
    - create_text_to_speech_interface()
    - create_speech_to_text_interface()
    - create_voice_session_panel()
    - display_multimodal_statistics()

# Helper functions
- add_multimodal_sidebar_section()
- create_multimodal_features_page()
```

## üöÄ Quick Start

### 1. Installation

```bash
# Core dependencies
pip install pillow python-docx PyPDF2

# Audio processing
pip install pydub librosa

# Video processing
pip install moviepy

# Speech processing
pip install google-cloud-texttospeech google-cloud-speech
pip install azure-cognitiveservices-speech
pip install openai
pip install elevenlabs

# Optional
pip install better-profanity punctuator
```

### 2. Basic Usage

```python
from multimodal_advanced import MultimodalManager
from voice_advanced import TextToSpeech, TTSConfig

# Process multimodal files
manager = MultimodalManager()
media_file = manager.process_file("image.jpg")
print(media_file.to_dict())

# Text-to-speech
config = TTSConfig(
    provider="google",
    voice_name="en-US-Neural2-A",
    language="en-US",
    speed=1.0
)
tts = TextToSpeech(config)
audio_bytes, metadata = tts.synthesize("Hello, world!")
```

### 3. Streamlit Integration

```python
# In your app.py
from multimodal_voice_integration import (
    MultimodalVoiceIntegrator,
    add_multimodal_sidebar_section,
    create_multimodal_features_page
)

# Add to sidebar
add_multimodal_sidebar_section()

# Or create full page
if selected == "Multimodal":
    create_multimodal_features_page()
```

## üìä Features Breakdown

### Image Processing
- ‚úÖ Format validation (PNG, JPEG, GIF, WebP, BMP)
- ‚úÖ EXIF metadata extraction (camera, ISO, aperture)
- ‚úÖ Intelligent resizing (maintains aspect ratio)
- ‚úÖ AI optimization (contrast, brightness, size)
- ‚úÖ Color space detection (RGB, RGBA, CMYK)

### Document Processing
- ‚úÖ PDF text extraction with page numbers
- ‚úÖ DOCX parsing preserving formatting
- ‚úÖ CSV analysis with column detection
- ‚úÖ JSON structure validation
- ‚úÖ Markdown parsing
- ‚úÖ Metadata preservation (author, created date)

### Audio Processing
- ‚úÖ Duration calculation
- ‚úÖ MFCC (Mel-frequency cepstral coefficients) analysis
- ‚úÖ RMS energy computation
- ‚úÖ Zero-crossing rate detection
- ‚úÖ Format conversion (MP3, WAV, OGG, FLAC)
- ‚úÖ Audio normalization (-20dBFS)

### Video Processing
- ‚úÖ Video metadata (FPS, resolution, duration)
- ‚úÖ Frame extraction at key points
- ‚úÖ Thumbnail generation with custom timestamps
- ‚úÖ Quality-based compression
- ‚úÖ Frame count calculation

### Text-to-Speech
- ‚úÖ 4 provider support (Google, ElevenLabs, Azure, OpenAI)
- ‚úÖ SSML support for prosody control
- ‚úÖ Multiple voices per provider (6+ voices)
- ‚úÖ Language support (12+ languages)
- ‚úÖ Speed/pitch control
- ‚úÖ Volume normalization
- ‚úÖ Profile-based voice selection (professional, friendly, etc.)

### Speech-to-Text
- ‚úÖ 3 provider support (Google, Azure, OpenAI Whisper)
- ‚úÖ Automatic punctuation
- ‚úÖ Profanity filtering
- ‚úÖ Context hints for accuracy
- ‚úÖ Language detection
- ‚úÖ Confidence scoring
- ‚úÖ Multiple audio format support

### Voice Sessions
- ‚úÖ Session creation and closure
- ‚úÖ Message logging with timestamps
- ‚úÖ Duration tracking
- ‚úÖ User preferences storage
- ‚úÖ Session statistics
- ‚úÖ History archiving

### Real-time Audio
- ‚úÖ Live recording
- ‚úÖ Chunk buffering
- ‚úÖ Audio level detection
- ‚úÖ Silence detection
- ‚úÖ Configurable sample rate

## üé® UI Components

### Streamlit Integration

**Multimodal Uploader**
```python
MultimodalVoiceIntegrator().create_multimodal_uploader()
# Displays:
# - Format guide
# - Type selector
# - File upload area
# - Progress indication
# - Metadata viewer
```

**Voice Settings**
```python
prefs = MultimodalVoiceIntegrator().create_voice_settings()
# Configures:
# - Language preference
# - Voice gender
# - Voice profile
# - Speech rate
# - Accent
# - Auto-speak toggle
```

**Text-to-Speech Interface**
```python
MultimodalVoiceIntegrator().create_text_to_speech_interface(prefs)
# Features:
# - Text input area
# - Provider selector
# - Format selector
# - Generate button
# - Audio preview
# - Download button
```

**Speech-to-Text Interface**
```python
MultimodalVoiceIntegrator().create_speech_to_text_interface(prefs)
# Features:
# - Audio uploader
# - Provider selector
# - Transcribe button
# - Text output
# - Copy to clipboard
```

**Voice Session Panel**
```python
MultimodalVoiceIntegrator().create_voice_session_panel()
# Controls:
# - Start session
# - Pause session
# - Stop session
# - Duration display
```

## üîß Configuration

### Environment Variables

```bash
# Google Cloud
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"

# Azure Speech
export AZURE_SPEECH_KEY="your-key"
export AZURE_SPEECH_REGION="eastus"

# ElevenLabs
export ELEVENLABS_API_KEY="your-key"

# OpenAI
export OPENAI_API_KEY="your-key"
```

### Voice Presets

```python
# Professional voice
tts_config = TTSConfig(
    provider="google",
    voice_name="en-US-Neural2-C",
    profile=VoiceProfile.PROFESSIONAL,
    speed=0.9,
    pitch=1.0
)

# Friendly voice
tts_config = TTSConfig(
    provider="elevenlabs",
    voice_name="bella",
    profile=VoiceProfile.FRIENDLY,
    speed=1.0,
    pitch=1.1
)

# Calm voice
tts_config = TTSConfig(
    provider="azure",
    voice_name="en-US-AriaNeural",
    profile=VoiceProfile.CALM,
    speed=0.85,
    pitch=0.95
)
```

## üìà Performance

### Benchmarks

| Operation | Time | Memory |
|-----------|------|--------|
| Image metadata extraction | ~50ms | ~10MB |
| PDF text extraction (10 pages) | ~500ms | ~50MB |
| Audio duration detection | ~100ms | ~5MB |
| Video frame extraction (5 frames) | ~2s | ~100MB |
| TTS (100 chars) | ~500ms | ~20MB |
| STT (10s audio) | ~3s | ~30MB |

### Optimization Tips

1. **Resize images** before processing
2. **Compress videos** for frame extraction
3. **Cache results** for repeated files
4. **Use session state** for Streamlit
5. **Batch process** when possible
6. **Monitor memory** for large videos

## üêõ Error Handling

```python
try:
    media_file = manager.process_file("image.jpg")
except Exception as e:
    print(f"Processing error: {e}")

try:
    audio_bytes, metadata = tts.synthesize("Hello")
except ImportError:
    print("Provider library not installed")
except ValueError:
    print("Invalid configuration")
```

## üìö Advanced Examples

### Multi-language Support

```python
# Translate and speak in different languages
languages = ["en-US", "es-ES", "fr-FR"]
for lang in languages:
    config = TTSConfig(
        provider="google",
        voice_name="neural",
        language=lang
    )
    tts = TextToSpeech(config)
    audio, _ = tts.synthesize("Hello, world!")
```

### Voice Session Analytics

```python
manager = VoiceSessionManager()
manager.create_session("session1", "user1")
manager.add_voice_message("session1", "Hello", 5.0, is_user=True)
manager.add_voice_message("session1", "Hi there!", 3.0, is_user=False)

stats = manager.get_session_stats("session1")
print(f"Total duration: {stats['duration']}s")
print(f"Messages: {stats['messages']}")
```

### Real-time Audio Processing

```python
processor = RealTimeAudioProcessor()
processor.start_recording()

for chunk in audio_stream:
    processor.add_audio_chunk(chunk)
    level = processor.get_audio_level(chunk)
    if level < 0.02:  # Silence detected
        print("Silence detected")

audio_data = processor.stop_recording()
```

## üîó Integration with Brain Modes

The multimodal features integrate seamlessly with the advanced brain:

```python
# Combine with advanced brain for context-aware processing
from brain_integration import BrainIntegrator
from multimodal_voice_integration import MultimodalVoiceIntegrator

brain = BrainIntegrator()
multimodal = MultimodalVoiceIntegrator()

# Process multimodal input
files = multimodal.create_multimodal_uploader()

# Send to brain with context
for file_info in files["files"]:
    query = f"Analyze this {file_info['type']}: {file_info['name']}"
    response, metadata = brain.process_query_with_advanced_brain(query)
```

## ‚ùì FAQ

**Q: Which audio formats are supported?**
A: MP3, WAV, M4A, OGG, FLAC. The system auto-converts between formats.

**Q: Can I use multiple TTS providers?**
A: Yes! Switch providers in TTSConfig. Different providers have different voice characteristics.

**Q: How do I cache TTS results?**
A: Use the CacheManager from brain_config.py or implement your own caching layer.

**Q: Does it support real-time streaming?**
A: The RealTimeAudioProcessor handles chunked audio. Full streaming TTS support coming soon.

**Q: How do I filter inappropriate content?**
A: Enable `enable_profanity_filter=True` in STTConfig. Uses better-profanity library.

**Q: Can I process videos longer than 10 minutes?**
A: Yes, but frame extraction may be slow. Use compression first.

## üìû Support

For issues or feature requests:
1. Check existing documentation
2. Review error messages
3. Verify API keys are set
4. Check library versions
5. Review logs in processing history

## üéì Next Steps

1. ‚úÖ Start with image/document processing
2. ‚úÖ Add text-to-speech to responses
3. ‚úÖ Enable speech-to-text for input
4. ‚úÖ Create voice sessions for continuity
5. ‚úÖ Monitor statistics and performance
6. ‚úÖ Optimize for your use case

---

**Created:** 2026-01-21
**Last Updated:** 2026-01-21
**Version:** 1.0
